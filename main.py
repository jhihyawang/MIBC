import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from tqdm import tqdm
import os
import argparse
from datetime import datetime

from dataloader import get_dataloaders
from utils import set_seed, plot_confusion_matrix, plot_training_curves, calculate_class_weights
from model import SiameseResNetRuleModel
import torch.nn.functional as F  

# ---------------- Exam Rule Loss ----------------
def exam_rule_loss(exam_log_prob, targets, class_weights=None):
    """
    exam_log_prob: (B,3) ä¾†è‡ª model çš„ exam-level log æ©Ÿç‡
    targets:   (B,)  exam-level labelï¼ˆ0/1/2ï¼‰
    class_weights: tensor(num_classes,) æˆ– None
    """
    # # è½‰æˆ log prob
    # log_p = torch.log(exam_probs)

    if class_weights is not None:
        return F.nll_loss(exam_log_prob, targets, weight=class_weights)
    else:
        return F.nll_loss(exam_log_prob, targets)


# ==========================================
# åƒæ•¸è§£æ
# ==========================================
def parse_args():
    parser = argparse.ArgumentParser(description='è¨“ç·´ Mammography å¤šè¦–è§’åˆ†é¡æ¨¡å‹')
    
    # è³‡æ–™é›†åƒæ•¸
    parser.add_argument('--csv_train', type=str, default='csv/six_classes/train_labels.csv',
                        help='è¨“ç·´é›† CSV è·¯å¾‘')
    parser.add_argument('--csv_val', type=str, default='csv/six_classes/val_labels.csv',
                        help='é©—è­‰é›† CSV è·¯å¾‘')
    parser.add_argument('--csv_test', type=str, default='csv/six_classes/test_labels.csv',
                        help='æ¸¬è©¦é›† CSV è·¯å¾‘')
    parser.add_argument('--root_dir', type=str, default='datasets_v1',
                        help='åœ–ç‰‡æ‰€åœ¨çš„æ ¹ç›®éŒ„')
    parser.add_argument('--img_height', type=int, default=1024,
                        help='å½±åƒé«˜åº¦')
    parser.add_argument('--img_width', type=int, default=512,
                        help='å½±åƒå¯¬åº¦')
    
    # æ¨¡å‹åƒæ•¸
    parser.add_argument('--backbone', type=str, default='resnet50',
                        choices=['resnet18','resnet50', 'resnet101', 'resnet22_nyu', 'efficientnet_b0', 'efficientnet_b3', 
                                'efficientnet_b5', 'convnext_tiny', 'convnext_small', 'convnext_base'],
                        help='éª¨å¹¹ç¶²è·¯é¸æ“‡')
    parser.add_argument('--pretrained', action='store_true', default=True,
                        help='æ˜¯å¦ä½¿ç”¨é è¨“ç·´æ¬Šé‡')
    parser.add_argument('--num_classes', type=int, default=6,
                        help='åˆ†é¡é¡åˆ¥æ•¸é‡')
    parser.add_argument('--architecture', type=str, choices=['baseline','ipsi','bi','cross_view'], default='cross_view', help='æ¨¡å‹æ¶æ§‹')
    parser.add_argument('--concate_method', type=str, choices=['concat','concat_linear','concat_mlp'], default='concat', help='å¤šè¦–è§’ç‰¹å¾µèåˆæ–¹å¼')
    parser.add_argument('--decision_rule', type=str, choices=['max','avg','rule'], default='max', help='exam-level æ±ºç­–è¦å‰‡')

    # è¨“ç·´
    parser.add_argument('--batch_size', type=int, default=8,
                        help='Batch size')
    parser.add_argument('--num_workers', type=int, default=4,
                        help='DataLoader workers æ•¸é‡')
    parser.add_argument('--num_epochs', type=int, default=50,
                        help='è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--lr', type=float, default=2e-4,
                        help='å­¸ç¿’ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4,
                        help='æ¬Šé‡è¡°æ¸›')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=2,
                        help='æ¢¯åº¦ç´¯ç©æ­¥æ•¸')
    parser.add_argument('--mixed_precision', action='store_true', default=False,
                        help='æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è¨“ç·´')
    parser.add_argument('--use_class_weights', action='store_true', default=False,
                        help='æ˜¯å¦åœ¨æå¤±å‡½æ•¸ä¸­ä½¿ç”¨é¡åˆ¥æ¬Šé‡')
    parser.add_argument('--use_weighted_sampler', action='store_true', default=False,
                        help='æ˜¯å¦ä½¿ç”¨åŠ æ¬Šéš¨æ©Ÿæ¡æ¨£å™¨')
    # NYU ç›¸é—œåƒæ•¸
    parser.add_argument('--nyu_weights_path', type=str, default=None,
                        help='NYU breast cancer classifier é è¨“ç·´æ¬Šé‡è·¯å¾‘')
    # å…¶ä»–åƒæ•¸
    parser.add_argument('--save_dir', type=str, default='experiments',
                        help='å¯¦é©—æ ¹ç›®éŒ„')
    parser.add_argument('--experiment_id', type=str, default=None,
                        help='å¯¦é©— ID (è‹¥ä¸æŒ‡å®šå‰‡è‡ªå‹•ç”Ÿæˆ)')
    parser.add_argument('--device', type=str, default='cuda',
                        choices=['cuda', 'cpu'],
                        help='é‹ç®—è¨­å‚™')
    parser.add_argument('--eval_only', action='store_true',
                        help='åƒ…é€²è¡Œæ¸¬è©¦é›†è©•ä¼°')
    parser.add_argument('--checkpoint', type=str, default=None,
                        help='è¼‰å…¥çš„ checkpoint è·¯å¾‘')
    parser.add_argument('--seed', type=int, default=42,
                        help='éš¨æ©Ÿç¨®å­')
    args = parser.parse_args()
    
    # è‡ªå‹•ç”Ÿæˆå¯¦é©— ID (æ ¹æ“šä¸»è¦é…ç½®)
    if args.experiment_id is None:
        timestamp = datetime.now().strftime("%m%d_%H%M")
        args.experiment_id = f"{args.backbone}_bs{args.batch_size}_lr{args.lr:.0e}_ep{args.num_epochs}_{timestamp}"
    
    # è¨­å®šè£ç½®
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("CUDA ä¸å¯ç”¨ï¼Œåˆ‡æ›åˆ° CPU")
        args.device = 'cpu'
    
    return args

def train_one_epoch(model, loader, criterion, optimizer, device, epoch, scaler, args):
    model.train()
    running_loss = 0.0
    running_cls_loss = 0.0
    all_preds = []
    all_labels = []
    
    accumulation_steps = args.gradient_accumulation_steps
    
    loop = tqdm(loader, desc=f"Epoch {epoch+1}/{args.num_epochs} [Train]")
    
    for batch_idx, (images, labels) in enumerate(loop):
        images = images.to(device)
        labels = labels.to(device)
        
        with torch.amp.autocast('cuda', enabled=args.mixed_precision):
            # â­ æ–°ï¼šmodel å›å‚³ exam_probs, left_logits, right_logits
            exam_log_prob, L_prob, R_prob, L_logits, R_logits = model(images)

            # 1. exam-level loss
            cls_loss = criterion(exam_log_prob, labels)

            loss = cls_loss 
            loss = loss / accumulation_steps
        
        scaler.scale(loss).backward()
        
        if (batch_idx + 1) % accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
        
        running_loss += loss.item() * accumulation_steps
        running_cls_loss += cls_loss.item()

        # â­ ç”¨ exam_log_prob å–é æ¸¬
        preds = torch.argmax(exam_log_prob, dim=1).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())
        
        loop.set_postfix(
            loss=loss.item() * accumulation_steps,
            cls_loss=cls_loss.item(),
        )

    if len(loader) % accumulation_steps != 0:
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()
        
    epoch_loss = running_loss / len(loader)
    epoch_cls_loss = running_cls_loss / len(loader)
    epoch_acc = accuracy_score(all_labels, all_preds)

    return epoch_loss, epoch_cls_loss, epoch_acc
def validate(model, loader, criterion, device, args, phase="Valid"):
    model.eval()
    running_loss = 0.0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        loop = tqdm(loader, desc=f"[{phase}]")
        for images, labels in loop:
            images = images.to(device)
            labels = labels.to(device)
            
            with torch.amp.autocast('cuda', enabled=args.mixed_precision):
                exam_log_prob, L_prob, R_prob, L_logits, R_logits = model(images)
                loss = criterion(exam_log_prob, labels)
            
            running_loss += loss.item()
            
            preds = torch.argmax(exam_log_prob, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())
            
    epoch_loss = running_loss / len(loader)
    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='macro')
    
    return epoch_loss, acc, f1, all_labels, all_preds

def test(model, loader, device, args, exp_dir):
    """æ¸¬è©¦é›†è©•ä¼°ï¼Œè¼¸å‡ºå®Œæ•´å ±å‘Š"""
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        loop = tqdm(loader, desc="[Test]")
        for images, labels in loop:
            images = images.to(device)
            labels = labels.to(device)
            
            with torch.amp.autocast('cuda', enabled=args.mixed_precision):
                exam_log_prob, L_prob, R_prob, L_logits, R_logits = model(images)
            
            preds = torch.argmax(exam_log_prob, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())

    # è¨ˆç®—æŒ‡æ¨™
    test_acc = accuracy_score(all_labels, all_preds)
    test_f1 = f1_score(all_labels, all_preds, average='macro')
    cm = confusion_matrix(all_labels, all_preds)
    report = classification_report(all_labels, all_preds, zero_division=0, digits=4)
    
    # æº–å‚™å ±å‘Šå…§å®¹
    report_content = []
    report_content.append("="*60)
    report_content.append("ğŸ“Š æ¸¬è©¦é›†æœ€çµ‚çµæœ")
    report_content.append("="*60)
    report_content.append(f"å¯¦é©— ID: {args.experiment_id}")
    report_content.append(f"éª¨å¹¹ç¶²è·¯: {args.backbone}")
    report_content.append(f"å½±åƒå°ºå¯¸: {args.img_height}x{args.img_width}")
    report_content.append(f"Batch Size: {args.batch_size}")
    report_content.append(f"å­¸ç¿’ç‡: {args.lr}")
    report_content.append(f"è¨“ç·´è¼ªæ•¸: {args.num_epochs}")
    report_content.append("-"*60)
    report_content.append(f"Test Accuracy: {test_acc:.4f}")
    report_content.append(f"Test Macro-F1: {test_f1:.4f}")
    report_content.append("\nè©³ç´°åˆ†é¡å ±å‘Š:")
    report_content.append(report)
    report_content.append("\næ··æ·†çŸ©é™£:")
    report_content.append(str(cm))
    report_content.append("="*60)
    
    # è¼¸å‡ºåˆ°çµ‚ç«¯
    for line in report_content:
        print(line)
    
    # å„²å­˜å ±å‘Šåˆ°æª”æ¡ˆ
    report_path = 'report3.txt'
    with open(report_path, 'a+', encoding='utf-8') as f:
        f.write('\n'.join(report_content))
    print(f"\nâœ… æ¸¬è©¦å ±å‘Šå·²å„²å­˜è‡³: {report_path}")
    
    cm_path = os.path.join(exp_dir, f'cm_{args.experiment_id}.png')
    plot_confusion_matrix(all_labels, all_preds, cm_path, args.num_classes, phase=f"{args.experiment_id}")
    
    return test_acc, test_f1, all_labels, all_preds

def main():
    args = parse_args()
    set_seed(args.seed)
    # å»ºç«‹å¯¦é©—ç›®éŒ„çµæ§‹
    exp_dir = os.path.join(args.save_dir, args.experiment_id)
    checkpoint_dir = os.path.join(exp_dir, 'checkpoints')
    
    os.makedirs(exp_dir, exist_ok=True)
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    print("\n" + "="*60)
    print("ğŸ¥ Mammography å¤šè¦–è§’åˆ†é¡è¨“ç·´ç³»çµ±")
    print("="*60)
    print(f"å¯¦é©— ID: {args.experiment_id}")
    print(f"å¯¦é©—ç›®éŒ„: {exp_dir}")
    print(f"éª¨å¹¹ç¶²è·¯: {args.backbone}")
    print(f"å½±åƒå°ºå¯¸: {args.img_height}x{args.img_width}")
    print(f"Batch Size: {args.batch_size} (æœ‰æ•ˆ: {args.batch_size * args.gradient_accumulation_steps})")
    print(f"è¨“ç·´è¼ªæ•¸: {args.num_epochs}")
    print(f"å­¸ç¿’ç‡: {args.lr}")
    print(f"è¨­å‚™: {args.device}")
    print(f"æ··åˆç²¾åº¦: {args.mixed_precision}")
    print(f"éš¨æ©Ÿç¨®å­: {args.seed}")
    print(f"num_workers: {args.num_workers}")
    print("="*60 + "\n")
    
    device = torch.device(args.device)
    
    # å„²å­˜å¯¦é©—é…ç½®
    config_path = os.path.join(exp_dir, 'config.txt')
    with open(config_path, 'w', encoding='utf-8') as f:
        f.write("å¯¦é©—é…ç½®\n")
        f.write("="*60 + "\n")
        for key, value in vars(args).items():
            f.write(f"{key}: {value}\n")
    print(f"âœ… å¯¦é©—é…ç½®å·²å„²å­˜è‡³: {config_path}\n")
    
    # 1. è¨ˆç®—æ¬Šé‡ (é€™ä¸€æ­¥è§£æ±ºæ‚¨çš„ä¸å¹³è¡¡å•é¡Œ)
    if os.path.exists(args.csv_train) and args.use_class_weights:
        class_weights = calculate_class_weights(args.csv_train, args.num_classes, device)
    else:
        print("no class weights used in CE.")

    # 2. æº–å‚™ DataLoader
    img_size = (args.img_height, args.img_width)
    train_loader, val_loader, test_loader = get_dataloaders(
        csv_path_train=args.csv_train,
        csv_path_val=args.csv_val,
        csv_path_test=args.csv_test,
        root_dir=args.root_dir,
        img_size=img_size,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        seed=args.seed,
        use_weighted_sampler=args.use_weighted_sampler
    )
    
    # 3. åˆå§‹åŒ–æ¨¡å‹
    model = SiameseResNetRuleModel(
        backbone_name=args.backbone, 
        pretrained=args.pretrained, 
        num_classes=args.num_classes,
        architecture=args.architecture,
        concate_method=args.concate_method,
        decision_rule=args.decision_rule
    )
    model = model.to(device)
    
    # è¼‰å…¥ NYU é è¨“ç·´æ¬Šé‡ (å¦‚æœä½¿ç”¨ resnet22_nyu)
    if args.backbone == 'resnet22_nyu' and args.nyu_weights_path:
        if os.path.exists(args.nyu_weights_path):
            model.load_nyu_pretrained(args.nyu_weights_path)
        else:
            print(f"âš ï¸  NYU æ¬Šé‡æª”æ¡ˆä¸å­˜åœ¨: {args.nyu_weights_path}")
            print("   å°‡ä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–çš„æ¬Šé‡")
    elif args.backbone == 'resnet22_nyu':
        print("âš ï¸  ä½¿ç”¨ resnet22_nyu ä½†æœªæŒ‡å®š --nyu_weights_pathï¼Œå°‡ä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–")
    
    # è¼‰å…¥ checkpoint (å¦‚æœæœ‰)
    if args.checkpoint:
        print(f"è¼‰å…¥ checkpoint: {args.checkpoint}")
        model.load_state_dict(torch.load(args.checkpoint, map_location=device))
        print("âœ… Checkpoint è¼‰å…¥æˆåŠŸ\n")
    
    # å¦‚æœåªæ˜¯è©•ä¼°æ¨¡å¼
    if args.eval_only:
        if test_loader is None:
            print("âŒ è©•ä¼°æ¨¡å¼éœ€è¦æ¸¬è©¦é›†")
            return
        if args.checkpoint is None:
            print("âŒ è©•ä¼°æ¨¡å¼éœ€è¦æŒ‡å®š checkpoint")
            return
        test(model, test_loader, device, args, exp_dir)
        return
    
    # 4. Loss Function for exam-rule
    if args.use_class_weights:
        print("ä½¿ç”¨é¡åˆ¥æ¬Šé‡æ–¼æå¤±å‡½æ•¸ä¸­ (exam rule loss)")
        # å»ºç«‹ä¸€å€‹ closureï¼ŒæŠŠ class_weights å›ºå®šä½
        def criterion(exam_probs, targets):
            return exam_rule_loss(exam_probs, targets, class_weights)
    else:
        def criterion(exam_probs, targets):
            return exam_rule_loss(exam_probs, targets, None)

    
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    scheduler = CosineAnnealingLR(optimizer, T_max=args.num_epochs)
    
    # æ··åˆç²¾åº¦è¨“ç·´ (AMP) - ç¯€çœè¨˜æ†¶é«”ä¸¦åŠ é€Ÿ
    scaler = torch.amp.GradScaler('cuda', enabled=args.mixed_precision)
    
    best_f1 = 0.0
    
    # è¨“ç·´æ­·å²è¨˜éŒ„
    history = {
        'train_loss': [],
        'train_cls_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': [],
        'val_f1': []
    }
    
    # 5. è¨“ç·´å¾ªç’°
    for epoch in range(args.num_epochs):
        # Train
        train_loss, train_cls_loss, train_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, device, epoch, scaler, args
        )
        
        # Valid
        val_loss, val_acc, val_f1, val_labels, val_preds = validate(
            model, val_loader, criterion, device, args, phase="Valid"
        )
        
        scheduler.step()
        
        # è¨˜éŒ„æ­·å²
        history['train_loss'].append(train_loss)
        history['train_cls_loss'].append(train_cls_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['val_f1'].append(val_f1)
        
        print(f"\nEpoch {epoch+1}/{args.num_epochs} Stats:")
        print(f"Train Loss: {train_loss:.4f} (Cls: {train_cls_loss:.4f} | Acc: {train_acc:.4f}")
        print(f"Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Macro-F1: {val_f1:.4f}")
        
        # æ¯ 5 å€‹ epoch å°å‡ºè©³ç´°å ±å‘Šä¸¦ç¹ªè£½é©—è­‰é›†æ··æ·†çŸ©é™£
        if (epoch + 1) % 5 == 0:
            print("\nClassification Report:")
            print(classification_report(val_labels, val_preds, zero_division=0))
            
        # å„²å­˜æœ€ä½³æ¨¡å‹ (ä»¥ Macro F1 ç‚ºæº–ï¼Œæ¯”è¼ƒèƒ½åæ˜ å°‘æ•¸é¡åˆ¥çš„è¡¨ç¾)
        if val_f1 > best_f1:
            best_f1 = val_f1
            save_path = os.path.join(checkpoint_dir, f"best_model.pth")
            torch.save(model.state_dict(), save_path)
            print(f"ğŸ”¥ New Best Model Saved! (F1: {best_f1:.4f})")
            
    # ç¹ªè£½è¨“ç·´æ›²ç·š
    plot_training_curves(history, save_dir=exp_dir, title=args.experiment_id)

    # 6. è¨“ç·´çµæŸå¾Œï¼Œåœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°
    if test_loader is not None:
        print("\n" + "="*60)
        print("ğŸ¯ é–‹å§‹åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°æœ€ä½³æ¨¡å‹...")
        print("="*60)
        # è¼‰å…¥æœ€ä½³æ¨¡å‹
        best_model_path = os.path.join(checkpoint_dir, "best_model.pth")
        model.load_state_dict(torch.load(best_model_path))
        test(model, test_loader, device, args, exp_dir)

if __name__ == "__main__":
    main()